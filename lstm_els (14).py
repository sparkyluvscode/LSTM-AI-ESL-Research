# -*- coding: utf-8 -*-
"""LSTM_ELS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t6gQZaN1zdddL31K9ocQ6kgJZHFjLreL
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import os
from google.colab import files
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive

ellipse_data = pd.read_csv("/content/drive/MyDrive/ESL Passion Project/ELLIPSE_Final_github_train.csv")
ellipse_data2 = pd.read_csv("/content/drive/MyDrive/ESL Passion Project/ELLIPSE_Final_github_test.csv")


print(len(ellipse_data) + len(ellipse_data2) )

df = pd.concat([ellipse_data,ellipse_data2],ignore_index=True)

df.head()

df.to_csv("/content/drive/MyDrive/ESL Passion Project/all_esl_essays.csv")

essays = df["full_text"]

essays = essays.tolist()
print(essays[:2])

all_text = "\n".join(essays)

chars = sorted(list(set(all_text)))

# char_to_idx: Maps each character to a number
char_to_index = {c: i for i,c in enumerate(chars)}

#idx_to_char: Reverse mapping (number back to character)
index_to_char = {i: c for i,c in enumerate(chars)}

vocab_size = len(chars)
print(vocab_size)

print(vocab_size)
print("Char to Index Dictionary")
print(char_to_index)
print("Index to Char Dictionary")
print(index_to_char)

#@title data is ALL of the text converted to encoded numeric form for the purpose of training -> numpy array format
data = np.array([char_to_index[ch] for ch in all_text])


sequence_length = 60

print(f"total characters in dataset: {len(data)}")


X = []
y = []

for i in range(len(data) - sequence_length):
  X.append(data[i:i+sequence_length])
  y.append(data[i + sequence_length])

X = np.array(X)
y = np.array(y)

#building train, val, test data
num_samples = len(X)
train_val_split = int(0.90*num_samples)
val_test_split = int(0.98*num_samples)


X_train = X[:train_val_split]
y_train = y[:train_val_split]

X_val = X[train_val_split:val_test_split]
y_val = y[train_val_split:val_test_split]

X_test = X[val_test_split:]
y_test = y[val_test_split:]

#@title One-hot encode
class CharDataset(torch.utils.data.Dataset):
    def __init__(self, X, y, vocab_size):
        self.X = X          # integer indices, shape (N, seq_len)
        self.y = y          # integer targets, shape (N,)
        self.vocab_size = vocab_size

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        seq = self.X[idx]                  # (seq_len,)
        one_hot = np.zeros((len(seq), self.vocab_size), dtype=np.float32)

        one_hot[np.arange(len(seq)), seq] = 1.0
        x_tensor = torch.from_numpy(one_hot)      # (seq_len, vocab_size)
        y_tensor = torch.tensor(self.y[idx]).long()
        return x_tensor, y_tensor

# @title building the LSTM


class CharRNN(nn.Module):
  def __init__(self, vocab_size, hidden_size=256,num_layers=2,dropout = 0.3):
    super(CharRNN, self).__init__()

    self.lstm = nn.LSTM(
        input_size=vocab_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        batch_first = True,
        dropout = dropout if num_layers > 1 else 0
        )

    self.fc = nn.Linear(hidden_size,vocab_size)


  def forward(self,x):
    lstm_out, (h,c) = self.lstm(x)

    last_output = lstm_out[:,-1,:]
    logits = self.fc(last_output)
    return logits

#@title Making our trained model generate text
def generate_text(my_model, starting_text, length=200,
                  sequence_length=60, temperature=1.0, device="cuda"):
    my_model.eval()
    encoded = [char_to_index.get(ch, 0) for ch in starting_text]

    if len(encoded) < sequence_length:
        encoded = [0] * (sequence_length - len(encoded)) + encoded
    encoded = encoded[-sequence_length:]

    generated = starting_text

    with torch.no_grad():
        for _ in range(length):
            x = np.zeros((1, sequence_length, vocab_size), dtype=np.float32)
            x[0, np.arange(sequence_length), encoded[-sequence_length:]] = 1.0
            x = torch.from_numpy(x).to(device)

            logits = my_model(x) / temperature
            probs = torch.softmax(logits, dim=1)
            next_idx = torch.multinomial(probs, 1).item()
            next_char = index_to_char[next_idx]

            generated += next_char
            encoded.append(next_idx)

    return generated

# @title Creating batches & loading them
batch_size = 256

train_dataset = CharDataset(X_train, y_train, vocab_size)
val_dataset   = CharDataset(X_val,   y_val,   vocab_size)
test_dataset  = CharDataset(X_test,  y_test,  vocab_size)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader   = DataLoader(val_dataset,   batch_size=batch_size)
test_loader  = DataLoader(test_dataset,  batch_size=512)

# @title Training the Model and storing Train/Val Loss (DONE)


# define Loss and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(my_model.parameters(),lr=0.001)

num_epochs =10
starting_epoch=20

# Training
for epoch in range(num_epochs):

  my_model.train()
  train_loss=0

  for X_batch, y_batch in train_loader:
    X_batch = X_batch.to(device)
    y_batch = y_batch.to(device)

    logits = my_model(X_batch)
    loss = criterion(logits,y_batch)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    train_loss+= loss.item()

  train_loss/= len(train_loader)
  train_losses.append(train_loss)


  my_model.eval()
  val_loss = 0

  with torch.no_grad():
    for X_batch, y_batch in val_loader:
      X_batch = X_batch.to(device)
      y_batch = y_batch.to(device)
      logits = my_model(X_batch)
      loss = criterion(logits,y_batch)

      val_loss+= loss.item()

    val_loss/=len(val_loader)
    val_losses.append(val_loss)

  if (epoch + 1) % 5 == 0:
    torch.save(my_model.state_dict(), '/content/drive/MyDrive/ESL Passion Project/checkpoint_epoch_{}.pth'.format(epoch+1))
    print(f"Checkpoint saved at epoch {starting_epoch+epoch+1}")
    sample = generate_text(my_model, "The government is",
                           length=150,
                           sequence_length=sequence_length,
                           temperature=0.8,
                           device=device)
    print(sample[:300])



  print(f"Epoch {starting_epoch + epoch+1}/{starting_epoch + num_epochs} - Train Loss: {train_loss:.4f}, Val loss: {val_loss:.4f} ")

#@title The model will start from these losses
print(f"Previous Train loss: {train_losses[-1]:.4f}")

print(f"Previous Validation loss: {(val_losses[-1]):.4f}")

print(f"Previous Train loss: {train_losses}")

print(f"Previous Validation loss: {val_losses}")

# @title Plots (DONT TOUCH)
# plot training & validation loss

epochs=np.arange(30)
print(epochs)

plt.figure(figsize=(10,5))
plt.plot(epochs,train_losses,label='Train Loss', marker = 'o')
plt.plot(epochs,val_losses,label = 'Validation Loss',marker='s' )
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training")
plt.grid(True)
plt.show()


sns.set_style("whitegrid")
plt.figure(figsize=(10,6))
sns.lineplot(x=epochs,y=train_losses, label='Train Loss', marker = 'o')
sns.lineplot(x=epochs,y=val_losses,label = 'Validation Loss',marker='s' )

# @title Loading the Saved Parameters in


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
best_model = CharRNN(vocab_size).to(device)
mid_model = CharRNN(vocab_size).to(device)

#best model is EPOCH 20
best_state_dict = torch.load("/content/drive/MyDrive/ESL Passion Project/checkpoint_epoch_5.pth")
best_model.load_state_dict(best_state_dict)

#mid model is EPOCH 15
mid_model_dict = torch.load("/content/drive/MyDrive/ESL Passion Project/checkpoint_epoch_15.pth")
mid_model.load_state_dict(mid_model_dict)

print("loaded model again")

# @title Test Accuracy: best_model got 72.8% accuracy
#Accuracy: 63% initial, 72% after 30 epochs, (~73% accuracy) 72.8% after 20 epochs, 72.7% after 15 epochs, ~72% after 10 epochs

# X_test_tensor | y_test_tensor

best_model.eval()
all_preds = []

with torch.no_grad():
  for X_batch, y_batch in test_loader:
    X_batch = X_batch.to(device)
    y_batch = y_batch.to(device)
    pred = best_model(X_batch)
    all_preds.append(pred)


all_preds = torch.cat(all_preds)
#argmax finds the index of the largest value from the predictions tensor
predictions = torch.argmax(all_preds,dim=1)

total_correct = (predictions == torch.tensor(y_test).to(device)).sum()
accuracy = total_correct.float() / len(y_test)
print("Test accuracy:", accuracy.item())



prompts = [
    "Most schools let students",
    "I am",
    "Today is",
    "School",
    "Your mom"


]

for prompt in prompts:
    output = generate_text(best_model, prompt, length=150,
                           sequence_length=sequence_length,
                           temperature=0.8,
                           device=device)

    midput = generate_text(mid_model, prompt, length=150,
                           sequence_length=sequence_length,
                           temperature=0.8,
                           device=device)


    print("\n" + "="*70)
    print(f"Prompt: '{prompt}'")
    print("="*70)
    print("Best model: ",output)
    print("Mid model: ",midput)

#train_losses, val_losses = [], []
print(train_losses)

# @title Sampled 5 ESL prompts with 20 responses to each, total 100
import random
esl_model = best_model

n_per_prompt = 20
rows = []

print(df["prompt"].value_counts()[:5])
print("-"*70)
prompts = ["Distance learning",
           "Success and failure",
           "Career commitment",
           "Being busy",
           "Impact of technology"]

for prompt in prompts:
  subset = df[df["prompt"]==prompt]

  sample = subset.sample(n=min(n_per_prompt,len(subset)),random_state=42)

  for _, row in sample.iterrows():
    rows.append({"prompt": prompt, "full_text": row["full_text"],"source": "human_esl"})

esl_real = pd.DataFrame(rows)
print(esl_real.head())
print(len(esl_real),"sampled essays")

esl_real.to_csv("esl_real_sampled.csv",index=False)

# @title Perplexity Score
first_essay_text = esl_real["full_text"].iloc[0]

from torch.nn.functional import log_softmax


def compute_complexity(text, model, char_to_index, vocab_size, sequence_length=60,device=device):
  indices = [char_to_index.get(ch,0) for ch in text]

  if len(indices) <= sequence_length:
    print(f"Warning: text too short ({len(indices)} chars)")
    return float("inf"), float("inf"),0

  total_nll = 0.0
  num_predictions = 0

  for t in range(sequence_length,len(indices)):
    context = indices[t-sequence_length:t]
    target = indices[t]

    x = np.zeros((1,sequence_length,vocab_size),dtype=np.float32)

    for i, index in enumerate(context):
      x[0,i,index] = 1.0

    x_tensor = torch.from_numpy(x).to(device)

    logits = model(x_tensor)

    log_probs = log_softmax(logits,dim=1)

    nll = -log_probs[0,target].item()
    total_nll += nll
    num_predictions +=1

  avg_nll = total_nll/num_predictions
  perplexity = np.exp(avg_nll)

  return perplexity, avg_nll, num_predictions

ppl, nll, n_chars = compute_complexity(
    first_essay_text,
    esl_model,
    char_to_index,
    vocab_size,
    sequence_length=60,
    device=device,
)

print("Perplexity:", ppl)
print("Avg NLL:", nll)
print("Num predictions:", n_chars)
print("Preview:", first_essay_text[:])

esl_real.head()

#@title Running Perplexity Score for our Natural ESL Data

# Perplexity is a measure of similarity in a way, we expect low perplexity between esl_model and ESL data since it's easily predictable.
# We expect high perplexity between esl_model and AI-generated writing, and mid perplexity for esl_model and native-ai writing

# low NLL -> confident, high NLL -> not confident

perplexities = []
avg_nll = []
num_chars_list = []

for idx, row in esl_real.iterrows():
  text = row["full_text"]

  p,n,l = compute_complexity(
      text,
      esl_model,
      char_to_index,
      vocab_size,
      60,
      device
  )

  perplexities.append(p)
  avg_nll.append(n)
  num_chars_list.append(l)

esl_real["perplexity"] = perplexities
esl_real["avg_nll"] = avg_nll
esl_real["num_chars_used"] = num_chars_list

print(esl_real[["prompt","perplexity"]])

print(esl_real["perplexity"].describe())

# all 5 prompts
prompts_for_experiment = esl_real["prompt"].unique().tolist()

print(prompts_for_experiment)


# both ai prompts
native_ai_prompt = "Write a 400‑word persuasive essay responding to the following prompt as a fluent native English‑speaking high school student in an honors or AP class. Use richer vocabulary, more complex sentences (with subordinate clauses), and varied transitions, while staying clear and well‑organized, and generate only the essay itself: "
esl_ai_prompt = "Write a 400‑word persuasive essay responding to the following prompt as a mid‑level ESL high school student (roughly B1). Use everyday vocabulary, some repeated phrases, and mostly simple sentences. Include a few grammar and punctuation mistakes (for example with articles, prepositions, or verb tense), but keep the overall meaning clear, and generate only the essay itself: "

# @title This Handcrafts any prompts
def make_prompt(prompt_text, type):
  if type == "native":
    return (f"{native_ai_prompt} \n \n {prompt_text}")
  elif type == "esl":
    return (f"{esl_ai_prompt} \n \n {prompt_text}")
  else:
    raise ValueError("style must be 'native' or 'esl'")

import openai, time
from google.colab import userdata

api_key = userdata.get('openai_key')

client = openai.OpenAI(api_key=api_key)

# for i in range(len(prompts_for_experiments)):
#.   for n in range(20):
prompt1 = make_prompt(prompts_for_experiment[0],"native")
print(prompt1)

# function that generates AI essay w/ API call
def generate_essay(prompt,type,model,temperature=0.8):
  prompt_for_ai = make_prompt(prompt,type)

  messages = [
      {"role":"system", "content": "You are a helpful writing tutor who writes persuasive high school essays following the user’s instructions"},
      {"role":"user","content":prompt_for_ai}
  ]

  response = client.chat.completions.create(model=model,messages=messages,temperature=temperature)

  essay_text = response.choices[0].message.content

  return essay_text

#@title Test: AI Generated Native Essay on "Distance Learning" Prompt
native_ai_essay = generate_essay(prompt1,"native",model="gpt-4.1")

print(native_ai_essay)

#@title Test: AI Generated ESL on "Distance Learning" Prompt
esl_ai_essay = generate_essay(prompt1,"esl",model="gpt-4.1")

print(len(esl_ai_essay.strip()))
print(esl_ai_essay.strip())

'''
Feb 7- Today I made the function to generate an AI essay. Now, what's next? We need to generate native AISAs for all five prompts, 20 times, for a total of 100. Then, do that same thing for ESL and compute complexity scores or perplexity scores.
'''

number_of_essays_per_prompt = 20

ai_rows = []

for prompt in prompts_for_experiment:
  for i in range(number_of_essays_per_prompt):


    native_ai_essay = generate_essay(prompt, "native",model="gpt-4.1")
    esl_ai_essay = generate_essay(prompt, "esl",model="gpt-4.1")

    ai_rows.append({
        "prompt":prompt,
        "full_text": native_ai_essay,
        "style": "native_ai"
    })

    ai_rows.append({
        "prompt":prompt,
        "full_text": esl_ai_essay,
        "style": "esl_ai"
    })

ai_df = pd.DataFrame(ai_rows)
ai_df["id"] = range(len(ai_df))
print(ai_df["style"].value_counts())
print(len(ai_df))  # should be 200
ai_df.to_csv("/content/drive/MyDrive/ESL Passion Project/ai_essays_5prompts_20each.csv", index=False)

#@title This is a generalizable function that uses compute_complexity to add perplexity scores to a DF given the name of it's column that contains text
def add_perplexity(df,text_col,model):

  perplexities=[]
  avg_nlls = []
  num_chars_list=[]

  for text in df[text_col]:
    p,n,l = compute_complexity(text,model,char_to_index,vocab_size,60,device)

    perplexities.append(p)
    avg_nlls.append(n)
    num_chars_list.append(l)


  df = df.copy()
  df["perplexity"] = perplexities
  df["avg_nll"] = avg_nlls
  df["num_chars_used"] = num_chars_list
  return df

ai_df.to_csv("/content/drive/MyDrive/ESL Passion Project/ai_essays_5prompts_20each.csv", index=False)
ai_scored = add_perplexity(ai_df,"full_text",esl_model)
ai_scored.to_csv("/content/drive/MyDrive/ESL Passion Project/ai_scored5prompts_20each.csv",index=False)

print("HUMAN ESL:")
print(esl_real["perplexity"].describe())

print("\nAI by style:")
print(ai_scored.groupby("style")["perplexity"].describe())

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.kdeplot(data=esl_real, x="perplexity", label="human_esl")
sns.kdeplot(data=ai_scored[ai_scored["style"]=="esl_ai"], x="perplexity", label="ai_esl")
sns.kdeplot(data=ai_scored[ai_scored["style"]=="native_ai"], x="perplexity", label="ai_native")
plt.legend()
plt.title("Perplexity under ESL LSTM")
plt.savefig("/content/drive/MyDrive/ESL Passion Project/perplexity_graph.png")
plt.show()

#@title Human vs. all AI

import numpy as np

# Labels
y_human = np.zeros(len(esl_real))              # 0 = human ESL
y_ai    = np.ones(len(ai_scored))              # 1 = AI (both styles)

all_ppl = np.concatenate([
    esl_real["perplexity"].values,
    ai_scored["perplexity"].values
])
all_y = np.concatenate([y_human, y_ai])

# Sweep thresholds
thresholds = np.linspace(all_ppl.min(), all_ppl.max(), 200)

best_acc = 0.0
best_t = None

for t in thresholds:
    y_pred = (all_ppl > t).astype(int)         # > t → AI
    acc = (y_pred == all_y).mean()
    if acc > best_acc:
        best_acc = acc
        best_t = t

print("Best threshold (human vs ALL AI):", best_t)
print("Best overall accuracy:", best_acc)

# False positive rate on human ESL at this threshold
y_pred_human = (esl_real["perplexity"].values > best_t).astype(int)
fp_rate = (y_pred_human == 1).mean()
print("False positive rate on human ESL:", fp_rate)

# True positive rate on AI at this threshold
y_pred_ai = (ai_scored["perplexity"].values > best_t).astype(int)
tp_rate = (y_pred_ai == 1).mean()
print("True positive rate on AI:", tp_rate)

#@title Human vs. AI native
ai_native = ai_scored[ai_scored["style"] == "native_ai"].copy()

y_human2  = np.zeros(len(esl_real))
y_native  = np.ones(len(ai_native))

all_ppl2 = np.concatenate([
    esl_real["perplexity"].values,
    ai_native["perplexity"].values
])
all_y2 = np.concatenate([y_human2, y_native])

thresholds2 = np.linspace(all_ppl2.min(), all_ppl2.max(), 200)

best_acc2 = 0.0
best_t2 = None

for t in thresholds2:
    y_pred2 = (all_ppl2 > t).astype(int)
    acc2 = (y_pred2 == all_y2).mean()
    if acc2 > best_acc2:
        best_acc2 = acc2
        best_t2 = t

print("\nBest threshold (human vs AI-native):", best_t2)
print("Best overall accuracy:", best_acc2)

y_pred_human2 = (esl_real["perplexity"].values > best_t2).astype(int)
fp_rate2 = (y_pred_human2 == 1).mean()
print("False positive rate on human ESL:", fp_rate2)

y_pred_native = (ai_native["perplexity"].values > best_t2).astype(int)
tp_rate2 = (y_pred_native == 1).mean()
print("True positive rate on AI-native:", tp_rate2)

print("\nMean perplexity per group:")
print("Human ESL:", esl_real["perplexity"].mean())
print(ai_scored.groupby("style")["perplexity"].mean())